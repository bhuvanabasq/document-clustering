{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f74Hp_RDlEXY"
   },
   "source": [
    "This colab notebook will help in understanding the different variations of BERT and how they respond to similarity metric like cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeowOP5Yr_yL"
   },
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1YXfqe3sGiJ",
    "outputId": "408c6a6a-a04c-45d4-aa82-9f97cf693ea2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rmaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rmaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# import umap\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Hr0t0Cf4JkG"
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JqcMy9Disz24"
   },
   "outputs": [],
   "source": [
    "def prepare_data(stopwords):\n",
    "    try: \n",
    "        df = pd.read_csv('data/raw_data.csv',encoding='ISO-8859-1')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    df = df[['Title','Abstract']]\n",
    "    df = df.dropna().drop_duplicates(subset=['Abstract'])\n",
    "    df = df.dropna().drop_duplicates(subset=['Title'])\n",
    "    df['Abstract'] = df['Abstract'].apply(remove_stop_lemmatize, args=(stopwords,))\n",
    "    df = df[df['Abstract'].str.len() > 150].reset_index()\n",
    "    df['index']    = df.index\n",
    "    df['abstract_list'] =  df['Abstract'].apply(lambda x : x.split(' '))\n",
    "    df['abstract_list_len'] =  df['abstract_list'].apply(lambda x : len(x))\n",
    "    df['abstract_list_trunc'] =  df['abstract_list'].apply(lambda x : x[0:400])\n",
    "    df['aabstract_list_trunc_len'] =  df['abstract_list_trunc'].apply(lambda x : len(x))\n",
    "    df['final_text'] = df['abstract_list_trunc'].apply(lambda x : ' '.join(x))\n",
    "    return df\n",
    "\n",
    "def remove_stop_lemmatize(text, stopwords):\n",
    "    text = text.replace('-',' ')\n",
    "    text = re.sub (r'([^a-zA-Z ]+?)', '', text)\n",
    "    if stopwords:\n",
    "        text = text.split(' ')\n",
    "        return ' '.join(lemmatizer.lemmatize(word.lower()) for word in text if word.lower() not in stop_words)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GGsdQYEkX5dU"
   },
   "outputs": [],
   "source": [
    "n_data = prepare_data(stopwords=False)\n",
    "text = n_data['final_text'][0]\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "# Print out the tokens.\n",
    "# n_data['final_text'] = \"[CLS] \" + n_data['final_text'] + \" [SEP]\"\n",
    "n_data['tokenized_marked'] = n_data['final_text'].apply((lambda x: tokenizer.tokenize(x)))\n",
    "n_data['tokenized'] = n_data['final_text'].apply((lambda x: tokenizer.encode(x)))\n",
    "max_len = 0\n",
    "for i in n_data['tokenized'].values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "n_data['tokenized_padded'] = n_data['tokenized'].apply((lambda x: x + ([0]*(max_len - len(x)))))\n",
    "n_data['hashed_tokens'] = n_data['tokenized_marked'].apply((lambda x: [i for i in x if \"##\" in i]))\n",
    "n_data['hashed_tokens_len'] = n_data['hashed_tokens'].apply((lambda x: len(x)))\n",
    "n_data['tokenized_padded_len'] = n_data['tokenized_padded'].apply((lambda x: len(x)))\n",
    "n_data['segment'] = n_data['tokenized'].apply((lambda x: [1 for i in range(len(x))]))\n",
    "n_data['segment_padded'] = n_data['segment'].apply((lambda x: x + ([0]*(max_len - len(x)))))\n",
    "n_data['segment_padded_len'] = n_data['segment_padded'].apply((lambda x: len(x)))\n",
    "# print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HGDIQtPgtF9"
   },
   "source": [
    "The original word has been split into smaller subwords and characters. This is because Bert Vocabulary is fixed with a size of ~30K tokens. Words that are not part of vocabulary are represented as subwords and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "n-BMUocngVGd",
    "outputId": "184ec99d-71ca-4246-afa7-4ccd3d2f8812"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([109., 154., 114.,  58.,  23.,   7.,   0.,   0.,   0.,   1.]),\n",
       " array([  0. ,  10.5,  21. ,  31.5,  42. ,  52.5,  63. ,  73.5,  84. ,\n",
       "         94.5, 105. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(n_data['hashed_tokens_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ1JZS8MjPU6"
   },
   "source": [
    "Next we need to convert our data to tensors(input format for the model) and call the BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tp0hBib0RpZ"
   },
   "source": [
    "# Checking various pooling schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "34tK5FaQ_ZJK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@  Similar pair of text  @@@@@@@@@@@@@@@@@@@@\n",
      "Title of paper DocBERT: BERT for Document Classification\n",
      "Title of paper Publicly Available Clinical BERT Embeddings\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper SciBERT: Pretrained Contextualized Embeddings for Scientific Text\n",
      "Title of paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\n",
      "Title of paper ELMo Meet BERT: Recent Advances in Natural Language Embeddings\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper WordRank: Learning Word Embeddings via Robust Ranking\n",
      "Title of paper Combining Contextualized Embeddings and Prior Knowledge for Clinical Named Entity Recognition: Evaluation Study\n",
      "---------------------------------------------------------------------------------\n",
      "@@@@@@@@@@@@@@@  Dissimilar pair of text   @@@@@@@@@@@@@@@@@@\n",
      "Title of paper DocBERT: BERT for Document Classification\n",
      "Title of paper Two public chest X-ray datasets for computer-aided screening of pulmonary diseases\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper Language Models are Few-Shot Learners\n",
      "Title of paper PadChest: A large chest x-ray image dataset with multi-label annotated reports\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation\n",
      "Title of paper Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection\n",
      "---------------------------------------------------------------------------------\n",
      "Title of paper DocBERT: BERT for Document Classification\n",
      "Title of paper Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# preparing the test data for the \n",
    "similar_set    = [[0,5],[17,57],[424,426],[120,121]]\n",
    "dissimilar_set = [[0,115],[26,28],[1,119],[0,1]]\n",
    "print('@@@@@@@@@@@@@@@@  Similar pair of text  @@@@@@@@@@@@@@@@@@@@')\n",
    "similar_dict = {}\n",
    "count = 1 \n",
    "for i in similar_set:\n",
    "    similar_dict[count] = {}\n",
    "    similar_dict[count][i[0]] = {}\n",
    "    similar_dict[count][i[1]] = {}\n",
    "    for j in i:\n",
    "        print('Title of paper {}'.format(n_data['Title'][j]))\n",
    "        similar_dict[count][j]['token']  = torch.tensor(np.array([i  for i in n_data['tokenized_padded'][j]]).reshape(1,len(n_data['tokenized_padded'][j]))).to(torch.int64) \n",
    "        similar_dict[count][j]['att']    = torch.tensor(np.array([i  for i in n_data['segment_padded'][j]]).reshape(1,len(n_data['segment_padded'][j]))).to(torch.int64) \n",
    "    count = count + 1\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "print('@@@@@@@@@@@@@@@  Dissimilar pair of text   @@@@@@@@@@@@@@@@@@')\n",
    "dissimilar_dict = {}\n",
    "count = 1 \n",
    "for i in dissimilar_set:\n",
    "    dissimilar_dict[count] = {}\n",
    "    dissimilar_dict[count][i[0]] = {}\n",
    "    dissimilar_dict[count][i[1]] = {}\n",
    "    for j in i:\n",
    "        print('Title of paper {}'.format(n_data['Title'][j]))\n",
    "        dissimilar_dict[count][j]['token']  = torch.tensor(np.array([i  for i in n_data['tokenized_padded'][j]]).reshape(1,len(n_data['tokenized_padded'][j]))).to(torch.int64) \n",
    "        dissimilar_dict[count][j]['att']    = torch.tensor(np.array([i  for i in n_data['segment_padded'][j]]).reshape(1,len(n_data['segment_padded'][j]))).to(torch.int64) \n",
    "    count = count + 1\n",
    "    print('---------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71omlfT24V_i",
    "outputId": "ccd6cd0b-b010-4856-90c0-e50aa2a4a27f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 503\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "\n",
    "model = model_class.from_pretrained(pretrained_weights,output_hidden_states = True)\n",
    "model.eval()\n",
    "print('Put the model in \"evaluation\" mode, meaning feed-forward operation.')\n",
    "tokens_tensor = torch.tensor(np.array([i  for i in n_data['tokenized_padded'][0]]).reshape(1,len(n_data['tokenized_padded'][0]))).to(torch.int64) \n",
    "segments_tensors = torch.tensor(np.array([i  for i in n_data['segment_padded'][0]]).reshape(1,len(n_data['segment_padded'][0])))\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "hidden_states  = outputs[2] #Hidden layer are nothing but encoders\n",
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Skf0yY8gQF9f"
   },
   "source": [
    "\n",
    "\n",
    "We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5rY6x6b0YtY"
   },
   "source": [
    "## Concating the last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q7hnLnGdmLLm"
   },
   "outputs": [],
   "source": [
    "def concat_ll(tokens_tensor, segments_tensors):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states  = outputs[2]\n",
    "    # Concatenating last four hidden layers\n",
    "    cat_vec  = torch.cat((hidden_states[-1][0], hidden_states[-2][0], hidden_states[-3][0], hidden_states[-4][0]), dim=1)[0]\n",
    "    # print(cat_vec.size())\n",
    "    return cat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9frqqEATzYsT",
    "outputId": "f1e40b1d-2092-42e7-d106-b1135b62daab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.918615, 0.91495407, 0.8795309, 0.92155516]\n",
      "mean of set is 0.9086637794971466\n"
     ]
    }
   ],
   "source": [
    "similarity_score_list = []\n",
    "for key, value in similar_dict.items():\n",
    "    temp = []\n",
    "    for key_2, value_2 in similar_dict[key].items():\n",
    "        temp.append(concat_ll(tokens_tensor = similar_dict[key][key_2]['token'], segments_tensors= similar_dict[key][key_2]['att']).reshape(1,3072))\n",
    "    similarity_score_list.append(cosine_similarity(temp[0], temp[1])[0][0])\n",
    "    \n",
    "print(similarity_score_list)\n",
    "print('mean of set is {}'.format(sum(similarity_score_list)/len(similarity_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N611VF9t0iBv",
    "outputId": "eed01b7b-4340-4dfd-f0de-87ee89dc67ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77199185, 0.8961681, 0.9235461, 0.92355657]\n",
      "mean of set is 0.8788156509399414\n"
     ]
    }
   ],
   "source": [
    "dissimilarity_score_list =[]\n",
    "for key, value in dissimilar_dict.items():\n",
    "    temp = []\n",
    "    for key_2, value_2 in dissimilar_dict[key].items():\n",
    "        temp.append(concat_ll(tokens_tensor = dissimilar_dict[key][key_2]['token'], segments_tensors= dissimilar_dict[key][key_2]['att']).reshape(1,3072))\n",
    "    dissimilarity_score_list.append(cosine_similarity(temp[0], temp[1])[0][0])\n",
    "print(dissimilarity_score_list)\n",
    "print('mean of set is {}'.format(sum(dissimilarity_score_list)/len(dissimilarity_score_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNZps9bm02Zj"
   },
   "source": [
    "## Taking mean of Last hidden Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xoLoQYb7yWm6"
   },
   "outputs": [],
   "source": [
    "def mean_ll(tokens_tensor, segments_tensors):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states  = outputs[2]\n",
    "    mean_vec  = torch.mean(hidden_states[-1][0], dim=0)\n",
    "  # print(mean_vec.size())\n",
    "    return mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9FajpfaxQc-",
    "outputId": "dc8d2cda-9dc9-4986-ba9c-a127e66aa690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92876005, 0.93804026, 0.91405797, 0.90509796]\n",
      "mean of set is 0.9214890599250793\n"
     ]
    }
   ],
   "source": [
    "similarity_score_list =[]\n",
    "for key, value in similar_dict.items():\n",
    "    temp = []\n",
    "    for key_2, value_2 in similar_dict[key].items():\n",
    "        temp.append(mean_ll(tokens_tensor = similar_dict[key][key_2]['token'], segments_tensors= similar_dict[key][key_2]['att']).reshape(1,768))\n",
    "    similarity_score_list.append(cosine_similarity(temp[0], temp[1])[0][0])\n",
    "print(similarity_score_list)\n",
    "print('mean of set is {}'.format(sum(similarity_score_list)/len(similarity_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frOJyxShFzEd",
    "outputId": "c621ba86-2569-4811-b9bc-8dd0d09316e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8232513, 0.87436557, 0.9059862, 0.93921506]\n",
      "mean of set is 0.8857045322656631\n"
     ]
    }
   ],
   "source": [
    "dissimilarity_score_list =[]\n",
    "for key, value in dissimilar_dict.items():\n",
    "    temp = []\n",
    "    for key_2, value_2 in dissimilar_dict[key].items():\n",
    "        temp.append(mean_ll(tokens_tensor = dissimilar_dict[key][key_2]['token'], segments_tensors= dissimilar_dict[key][key_2]['att']).reshape(1,768))\n",
    "    dissimilarity_score_list.append(cosine_similarity(temp[0], temp[1])[0][0])\n",
    "print(dissimilarity_score_list)\n",
    "print('mean of set is {}'.format(sum(dissimilarity_score_list)/len(dissimilarity_score_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivszval91HAB"
   },
   "source": [
    "## Last Hidden layer [CLS] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EkyBMjj91JYW"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    df_bbw = pd.read_csv('data/Bert_base_weights.csv',header=None)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "for i in range(768):\n",
    "    df_bbw[i] = df_bbw[i].apply(lambda x : float(str(x).split(',')[0].replace('tensor(','')))\n",
    "\n",
    "bert_base_embeddings = np.array(df_bbw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylegq65AREDE",
    "outputId": "2daaaf61-bb87-4ec8-b7e3-7e7e222f48f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.        , 0.96574789],\n",
      "       [0.96574789, 1.        ]]), array([[1.        , 0.96148354],\n",
      "       [0.96148354, 1.        ]]), array([[1.        , 0.89703129],\n",
      "       [0.89703129, 1.        ]]), array([[1.        , 0.96220083],\n",
      "       [0.96220083, 1.        ]])]\n",
      "mean of set is [[1.         0.94661589]\n",
      " [0.94661589 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_score_list = []\n",
    "\n",
    "for i in similar_set:\n",
    "    a = []\n",
    "    a.append(bert_base_embeddings[i[0]])\n",
    "    a.append(bert_base_embeddings[i[1]])\n",
    "    similarity_score_list.append(cosine_similarity(a))\n",
    "\n",
    "print(similarity_score_list)\n",
    "print('mean of set is {}'.format(sum(similarity_score_list)/len(similarity_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuN4ohUJ6qKV",
    "outputId": "61547ce3-eae4-49b9-a621-b67c1e6189d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.        , 0.87646429],\n",
      "       [0.87646429, 1.        ]]), array([[1.        , 0.92396005],\n",
      "       [0.92396005, 1.        ]]), array([[1.        , 0.95796996],\n",
      "       [0.95796996, 1.        ]]), array([[1.        , 0.97760222],\n",
      "       [0.97760222, 1.        ]])]\n",
      "mean of set is [[1.         0.93399913]\n",
      " [0.93399913 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "dissimilarity_score_list = []\n",
    "for i in dissimilar_set:\n",
    "    a = []\n",
    "    a.append(bert_base_embeddings[i[0]])\n",
    "    a.append(bert_base_embeddings[i[1]])\n",
    "    dissimilarity_score_list.append(cosine_similarity(a))\n",
    "\n",
    "print(dissimilarity_score_list)\n",
    "print('mean of set is {}'.format(sum(dissimilarity_score_list)/len(dissimilarity_score_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTbS_iPk6tcx"
   },
   "source": [
    "## SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lzNro1T_IXe2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91118bf6e4e649ce918ed33dc3f9590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244715968.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "s_bert_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "s_bert_model.max_seq_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6X61Yn_lI453"
   },
   "outputs": [],
   "source": [
    "similarity_score_list = []\n",
    "for i in similar_set:\n",
    "    a= []\n",
    "    a.append(s_bert_model.encode(n_data['final_text'].iloc[i[0]]))\n",
    "    a.append(s_bert_model.encode(n_data['final_text'].iloc[i[1]]))\n",
    "    similarity_score_list.append(cosine_similarity(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeggVBADKdw5",
    "outputId": "bf2ca40d-c68b-47d9-b471-02ed0dcea799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.0000002, 0.8064753],\n",
      "       [0.8064753, 0.9999998]], dtype=float32), array([[1.0000001 , 0.74272776],\n",
      "       [0.74272776, 1.        ]], dtype=float32), array([[1.0000001 , 0.57347536],\n",
      "       [0.57347536, 0.9999999 ]], dtype=float32), array([[1.0000001, 0.7342792],\n",
      "       [0.7342792, 1.0000001]], dtype=float32)]\n",
      "mean of set is [[1.0000001  0.71423936]\n",
      " [0.71423936 0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_score_list)\n",
    "print('mean of set is {}'.format(sum(similarity_score_list)/len(similarity_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ryE26KaUKKQZ"
   },
   "outputs": [],
   "source": [
    "dissimilarity_score_list = []\n",
    "for i in dissimilar_set:\n",
    "    a= []\n",
    "    a.append(s_bert_model.encode(n_data['final_text'].iloc[i[0]]))\n",
    "    a.append(s_bert_model.encode(n_data['final_text'].iloc[i[1]]))\n",
    "\n",
    "    dissimilarity_score_list.append(cosine_similarity(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pB4F2fqTKq74",
    "outputId": "17bf428c-be4c-43cf-c546-fd5b1cdc1bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.0000002, 0.3565308],\n",
      "       [0.3565308, 0.9999995]], dtype=float32), array([[1.0000001, 0.6481657],\n",
      "       [0.6481657, 1.0000002]], dtype=float32), array([[0.99999976, 0.6214295 ],\n",
      "       [0.6214295 , 1.        ]], dtype=float32), array([[1.0000002 , 0.37555385],\n",
      "       [0.37555385, 0.99999976]], dtype=float32)]\n",
      "mean of set is [[1.0000001 0.50042  ]\n",
      " [0.50042   0.9999999]]\n"
     ]
    }
   ],
   "source": [
    "print(dissimilarity_score_list)\n",
    "print('mean of set is {}'.format(sum(dissimilarity_score_list)/len(dissimilarity_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHbTOp0x5-kd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BERT_base_knowledge_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
